
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>RANRAC: Robust Neural Scene Representations via Random Ray Consensus</title>

    <meta name="description" content="A robust framework for single-shot reconstruction and photo-realistic reconstruction from occluded input perspectives." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="TODOTeaserImagehttps://lfranke.github.io/vet/img/vet_teaser_rs.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:title" content="RANRAC: Robust Neural Scene Representations via Random Ray Consensus" />
    <meta property="og:description" content="A robust framework for single-shot reconstruction and photo-realistic reconstruction from occluded input perspectives." />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://bennobuschmann.com/ranrac" />
    <meta name="twitter:title" content="RANRAC: Robust Neural Scene Representations via Random Ray Consensus" />
    <meta name="twitter:description" content="A robust framework for single-shot reconstruction and photo-realistic reconstruction from occluded input perspectives." />
    <meta name="twitter:image" content="TODOTeaserImagehttps://lfranke.github.io/vet/img/vet_teaser_smallrs.jpg" />


<!--
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
//-->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>

    <link rel="stylesheet" href="css/dics.min.css">
    <script src="js/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
</head>

<body>
    <div class="container" id="main">
        <a href="..">Back to Website</a>
        <br><br>
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>RANRAC</b>: Robust Neural Scene Representations</br> via Random Ray Consensus</br>
                <small>
                    arXiv Preprint
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a style="text-decoration:none" href="https://bennobuschmann.com">
                            Benno Buschmann
                        </a>
                        <br>TU Delft &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://andreeadogaru.github.io/">
                            Andreea Dogaru
                        </a>
                        <br>FAU Erlangen-Nürnberg
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://graphics.tudelft.nl/~eisemann/">
                            Elmar Eisemann
                        </a>
                        <br>TU Delft &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://graphics.tudelft.nl/michael-weinmann/">
                            Michael Weinmann
                        </a>
                        <br>TU Delft &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://eggerbernhard.ch/">
                            Bernhard Egger
                        </a>
                        <br>FAU Erlangen-Nürnberg
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2312.09780">
                            <img src="img/paperIcon.png" height="60px" alt="Paper">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="TODO https://youtu.be/foobar">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="TODO https://github.com/ladida">
                            <img src="img/github.png" height="60px" alt="Code">
                                <h4><strong>Code<br>(Coming Soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                                <img src="img/data_icon.png" height="60px" alt="Data">
                                <h4><strong>Data<br>(Coming Soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                    <video id="v1" width="100%" autoplay loop muted controls>
                        <source src="img/RANRAC.mp4" type="video/mp4" />
                    </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    We introduce RANRAC, a robust reconstruction algorithm for 3D objects handling occluded and distracted images, which is a particularly challenging scenario that prior robust reconstruction methods cannot deal with. Our solution supports single-shot reconstruction by involving light-field networks, and is also applicable to photo-realistic, robust, multi-view reconstruction from real-world images based on neural radiance fields. While the algorithm imposes certain limitations on the scene representation and, thereby, the supported scene types, it reliably detects and excludes inconsistent perspectives, resulting in clean images without floating artifacts. Our solution is based on a fuzzy adaption of the random sample consensus paradigm, enabling its application to large scale models. We interpret the minimal number of samples to determine the model parameters as a tunable hyperparameter. This is applicable, as a cleaner set of samples improves reconstruction quality. Further, this procedure also handles outliers. Especially for conditioned models, it can result in the same local minimum in the latent space as would be obtained with a completely clean set. We report significant improvements for novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the baseline.
                </p>
            </div>
        </div>

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--&lt;!&ndash;                <div class="col-md-8 col-md-offset-2">&ndash;&gt;-->
<!--                    <video id="v1" width="100%" autoplay loop muted controls>-->
<!--                        <source src="img/RANRAC.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--&lt;!&ndash;                </div>&ndash;&gt;-->
<!--&lt;!&ndash;            <div class="row">&ndash;&gt;-->
<!--&lt;!&ndash;                <div class="text-center">&ndash;&gt;-->
<!--&lt;!&ndash;                    <div style="position:relative;padding-top:56.25%;">&ndash;&gt;-->
<!--&lt;!&ndash;                        <iframe src="TODO https://www.youtube-nocookie.com/embed/foobar" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>&ndash;&gt;-->
<!--&lt;!&ndash;                    </div>&ndash;&gt;-->
<!--&lt;!&ndash;                </div>&ndash;&gt;-->
<!--&lt;!&ndash;            </div>&ndash;&gt;-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Pipeline
                </h3>
                Overview of our robust reconstruction pipeline with RANRAC applied to light field networks [Sitzmann 2021].
                <br>
                <br>
                <img src="img/AlgorithmIllustration.svg" style="width:100%; height: 100%; overflow: visible">
                <br>
                <p class="text-justify" style="padding-top: 10px;">
                    The RANRAC algorithm for LFNs samples random hypotheses by selecting a set of random samples from the given perspective (a), and inferring the latent representation of these rays using the autodecoder of a pretrained LFN (b).
                    The obtained light field is then used to predict an image from the input perspective (c).
                    Based on this prediction, confidence in the random hypothesis is evaluated via the Euclidean distance between the predicted ray colors and the remaining color samples of the input image. The amount of samples which are explained by each hypothesis up to some margin are used to determine the best hypothesis (d).
                    All samples explained by the selected hypothesis are used for a final inference with the LFN to obtain the final model and latent representation (e).
                </p>
<!--                <br>-->

                <img src="img/RANRAC_Website_AlgIllusAnim.gif" style="width:100%; height: 100%; overflow: visible; padding-top: 0px;">

                <br>
                <p class="text-justify" style="padding-top: 10px">
                    For the application to Neural Radiance Fields, enabling photo-realistic reconstruction from real-world data, the pipeline is adapted: The sampling takes place in image space and the evaluation becomes a two-step process.
<!--                <br>-->
<!--                TODO Text about Nerf Evaluation two step-->
<!--                <br>-->
<!--                <img src="TODO" style="width:100%; height: 100%; overflow: visible" alt="NeRF Evaluation Illustration">-->
<!--                <br>-->
                </p>
            </div>
        </div>
        <br>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    RANRAC applied to Light Field Networks
                </h3>
                <p class="text-justify">
                    With the application of RANRAC to light field networks [Sitzmann 2021] we enable reconstruction from a single occluded input perspective.
                </p>

                <table class="table" style="table-layout: fixed">
                    <tr>
                        <th class="text-center">Occlusion</th>
                        <th class="text-center">Input</th>
                        <th class="text-center">LFN</th>
                        <th class="text-center">RANRAC (Ours)</th>
                        <th class="text-center">Ground Truth</th>
                        <th class="text-center">Consensus Set</th>
                    </tr>
                    <tr>
                        <td class="text-center">0%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/occ0_input.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/cleanlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/cleanours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/cleangt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/occ0_mask.png"></td>
                    </tr>

                    <tr>
                        <td class="text-center">5%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/imgocc5.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_5_carlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_5_carours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_5_cargt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/maskimgocc5.png"></td>
                    </tr>

                    <tr>
                        <td class="text-center">15%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/imgocc15.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_15_carlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_15_carours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_15_cargt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/maskimgocc15.png"></td>
                    </tr>

                    <tr>
                        <td class="text-center">25%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/imgocc25.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_25_carlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_25_carours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_25_cargt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/maskimgocc25.png"></td>
                    </tr>

                    <tr>
                        <td class="text-center">35%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/imgocc35.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_35_carlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_35_carours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_35_cargt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/maskimgocc35.png"></td>
                    </tr>

                    <tr>
                        <td class="text-center">45%</td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/imgocc45.png"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_45_carlfn.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_45_carours.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/final_imageocc_45_cargt.gif"></td>
                        <td class="text-center"><img style="width:85%" src="img/lfn_occ_streak/maskimgocc45.png"></td>
                    </tr>
                </table>

                <p class="text-justify">
                    RANRAC enables single-shot multi-class reconstruction from occluded input perspectives.
                </p>
                <br>
                <table class="table" style="table-layout: fixed">
                    <tr>
                        <th class="text-center">Input</th>
                        <th class="text-center">LFN</th>
                        <th class="text-center">RANRAC (Ours)</th>
                        <th class="text-center">Ground Truth</th>
                    </tr>
                    <tr>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/car.png"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/car_lfn.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/car_ours.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/car_gt.gif"></td>
                    </tr>
                    <tr>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane.png"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane_lfn.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane_ours.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane_gt.gif"></td>
                    </tr>
                    <tr>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/chair.png"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/chair_lfn.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/chair_ours.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/chair_gt.gif"></td>
                    </tr>
                    <tr>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane2.png"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane2_lfn.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane2_ours.gif"></td>
                        <td class="text-center"><img style="width:50%" src="img/lfn_qualli/plane2_gt.gif"></td>
                    </tr>
                </table>
            </div>
        </div>
        <br>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    RANRAC applied to Neural Radiance Fields
                </h3>
                RANRAC removes the artifacts caused by occlusions (left) in NeRF reconstructions [Mildenhall 2020, Müller 2022]. Compared to robustNeRF [Sabour 2023], more details are preserved in the reconstruction (right).
                <br>

                <div class="row">
                    <div class="col-md-6 col-md-offset-0">
                        <div class="b-dics" style="width: 100%">
                            <img class="NerfCompGif" src="img/nerf_comp/VanillaOcc20_nofirst30_loop10ms.gif" alt="NeRF" />
                            <img class="NerfCompGif" src="img/nerf_comp/RANSACocc20_nofirst30loop10ms_.gif" alt="RANRAC (Ours)" />
                        </div>
                        <small>(click and drag to swipe)</small>
                    </div>
                    <div class="col-md-6 col-md-offset-0">
                        <div class="b-dics" style="width: 100%">
                            <img class="NerfCompGif" src="img/nerf_comp/RobNeRFocc20_nofrst30_loop10ms.gif" alt="RobustNeRF (Sabour et al.)" />
                            <img class="NerfCompGif" src="img/nerf_comp/RANSACocc20_nofirst30loop10ms_.gif" alt="RANRAC (Ours)" />
                        </div>
                        <small>(click and drag to swipe)</small>
                    </div>
                </div>
                <br>

                <div style="padding-top: 10px">
                    <img src="img/NerfComparisionWithHighlights.png" style="width:100%" />
                    <p class="text-justify" style="margin-top: 10px">
                       Minor artifacts are best observable when taking a detailed look at individual frames. The inaccuracies RobustNeRF shows compared to RANRAC appear mostly at concavities.
                    </p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-12 col-md-offset-0">
<textarea id="bibtex" class="form-control" readonly>
@article{buschmann2023ranrac,
title={RANRAC: Robust Neural Scene Representations via Random Ray Consensus},
author={Buschmann, Benno and Dogaru, Andreea and Eisemann, Elmar and Weinmann, Michael and Egger, Bernhard},
journal={arXiv preprint arXiv:2312.09780},
year={2023}
}
</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors thank everyone involved with the project. The website template was sourced from  <a href="https://lfranke.github.io/vet/">VET</a>, who adapted from <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>, who borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                    Image sliders are from <a href="https://bakedsdf.github.io/">BakedSDF</a>.
                </p>
            </div>
        </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                References
            </h3>
            <div class="content has-text-justified">
                <p>[Sitzmann 2021] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand. "Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering." NeurIPS (2021)</p>
                <p>[Sabour 2023] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J. Fleet, and Andrea Tagliasacchi. "RobustNeRF: Ignoring Distractors With Robust Losses." CVPR (2023)</p>
                <p>[Mildenhall 2020] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis." ECCV (2020)</p>
                <p>[Müller 2022] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. "Instant neural graphics primitives with a multiresolution hash encoding." SIGGRAPH (2022)</p>
                <p>[Fischler 1981] Martin A Fischler and Robert C Bolles. "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography." Communications of the ACM, 24(6):381–395 (1981)
            </div>
        </div>
    </div>

</div>
</body>
</html>
